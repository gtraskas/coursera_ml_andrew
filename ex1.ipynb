{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "***\n",
    "## Linear Regression with One Variable\n",
    "\n",
    "Read the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "data1 = pd.read_csv('ex1data1.txt', names=['Population', 'Profit'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data\n",
    "\n",
    "Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.grid()\n",
    "plt.plot(data1.Population, data1.Profit, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Fit the linear regression parameters $\\theta$ to the dataset using gradient descent.\n",
    "\n",
    "#### Update Equations\n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$J(\\theta)=\\frac{1}{2m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "where the hypothesis $h_{\\theta}(x)$ is given by the linear model\n",
    "\n",
    "$h_{\\theta}(x)=\\theta^{T} x=\\theta_0+\\theta_1x_1$\n",
    "\n",
    "The parameters of the model are the $\\theta_j$ values. These values will be adjusted to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "\n",
    "$\\theta_j:=\\theta_j-\\alpha\\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (Simultaneously update $\\theta_j$ for all $j$)\n",
    "\n",
    "With each step of gradient descent, the parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Add another one dimension to the data to accommodate the $\\theta_0$ intercept term and initialize $\\theta_j$ to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features.\n",
    "n = len(data1.columns)-1 # subtract the target column\n",
    "\n",
    "# Create a function to pepare the data.\n",
    "def prepareData(data, n):\n",
    "    \"\"\"\n",
    "    Add 1s column, convert to matrices,\n",
    "    initialize theta.\n",
    "    Args:\n",
    "        data: read the data file\n",
    "        n: int\n",
    "    Returns:\n",
    "        x: a m by n+1 matrix\n",
    "        y: a m by 1 vector\n",
    "        theta: a n+1 by 1 vector\n",
    "    \"\"\"\n",
    "    # Add a column with 1s in the data set.\n",
    "    data.insert(0, 'Ones', 1)\n",
    "\n",
    "    # Define X and y, separating the data set.\n",
    "    x = data.iloc[:, 0:n+1]\n",
    "    y = data.iloc[:, n+1:n+2]\n",
    "\n",
    "    # Convert to matrices and initialize parameters theta to 0s.\n",
    "    # Theta is a vector [n + 1 x 1] and Theta Transpose is a vector [1 x n+1],\n",
    "    # where n is the number of features.\n",
    "    x = np.matrix(x.values)\n",
    "    y = np.matrix(y.values)\n",
    "    theta = np.matrix(np.zeros((n+1, 1)))\n",
    "    return x, y, theta\n",
    "\n",
    "x, y, theta = prepareData(data1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the learning rate $\\alpha$ to 0.01 and the iterations to 1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for iterations and learning rate α.\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "# Check the dimensions of the matrices.\n",
    "x.shape, y.shape, theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Cost $J(\\theta)$\n",
    "\n",
    "Performing gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor also the convergence by computing the cost. Implement a function to calculate $J(\\theta)$ and check the convergence of gradient descent implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute cost.\n",
    "def computeCost(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function.\n",
    "    Args:\n",
    "        x: a m by n+1 matrix\n",
    "        y: a m by 1 vector\n",
    "        theta: a n+1 by 1 vector\n",
    "    Returns:\n",
    "        cost: float\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    cost = np.sum(np.square((x * theta) - y)) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "computeCost(x, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Implement gradient descent with a loop structure. The cost $J(\\theta)$ is parameterized by the vector $\\theta$, not $x$ and $y$. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing $x$ or $y$. A good way to verify that gradient descent is working correctly is to look at the value of $J(\\theta)$ and check that it is decreasing with each step. After the correct implementation of gradient descent and computeCost, the value of $J(\\theta)$ should never increase, and should converge to a steady value by the end of the algorithm. The final parameters will be used to plot the linear fit and make predictions on profits in areas of 35,000 and 70,000 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to implement gradient descent.\n",
    "def gradientDescent(x, theta, iterations):\n",
    "    \"\"\"\n",
    "    Implement gradient descent.\n",
    "    Args:\n",
    "        x: a m by n+1 matrix\n",
    "        theta: a n+1 by 1 vector\n",
    "    Returns:\n",
    "        theta: a n+1 by 1 vector\n",
    "        J_vals: a #iterations by 1 vector\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    J_vals = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        error = (x * theta) - y\n",
    "        for j in range(len(theta.flat)):\n",
    "            theta.T[0, j] = theta.T[0, j] - (alpha/m) * np.sum(np.multiply(error, x[:, j]))\n",
    "        J_vals.append(computeCost(x, y, theta))\n",
    "    return (theta, J_vals)\n",
    "\n",
    "theta, J_vals = gradientDescent(x, theta, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Fit Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_f = list(theta.flat)\n",
    "xs = np.arange(5, 23)\n",
    "ys = theta_f[0] + theta_f[1] * xs\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.grid()\n",
    "plt.plot(data1.Population, data1.Profit, 'rx', label='Training Data')\n",
    "plt.plot(xs, ys, 'b-', label='Linear Regression: h(x) = %0.2f + %0.2fx'%(theta[0], theta[1]))\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the profit for population of 35000 and 70000.\n",
    "print((theta_f[0] + theta_f[1] * 3.5) * 10000)\n",
    "print((theta_f[0] + theta_f[1] * 7) * 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing $J(\\theta)$\n",
    "\n",
    "Plot the cost over a 2-dimensional grid of $\\theta_0$ and $\\theta_1$ values to better understand the cost function $J(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# Create meshgrid.\n",
    "xs = np.arange(-10, 10, 0.4)\n",
    "ys = np.arange(-2, 5, 0.14)\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "\n",
    "# Initialize J values to a matrix of 0's.\n",
    "J_vals = np.zeros((xs.size, ys.size))\n",
    "\n",
    "# Fill out J values.\n",
    "for index, v in np.ndenumerate(J_vals):\n",
    "    J_vals[index] = computeCost(x, y, [[xx[index]], [yy[index]]])\n",
    "\n",
    "# Create a set of subplots.\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "# Create surface plot.\n",
    "ax1.plot_surface(xx, yy, J_vals, alpha=0.5, cmap='jet')\n",
    "ax1.set_zlabel('Cost', fontsize=14)\n",
    "ax1.set_title('Surface plot of cost function')\n",
    "\n",
    "# Create contour plot.\n",
    "ax2.contour(xx, yy, J_vals, np.logspace(-2, 3, 20), cmap='jet')\n",
    "ax2.plot(theta_f[0], theta_f[1], 'rx')\n",
    "ax2.set_title('Contour plot of cost function, showing minimum')\n",
    "\n",
    "# Create labels for both plots.\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r'$\\theta_0$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\theta_1$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Multiple Variables\n",
    "\n",
    "The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('ex1data2.txt', names=['Size', 'Bedrooms', 'Price'])\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Load and display some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly:\n",
    "\n",
    "* Subtract the mean value of each feature from the dataset.\n",
    "* After subtracting the mean, additionally scale (divide) the feature values by their respective standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features, but NOT target price!\n",
    "data2.iloc[:, 0:2] = data2.iloc[:, 0:2].apply(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Previously, gradient descent was implemented on a univariate regression problem. The only difference now is that there is one more feature in the matrix $x$. The hypothesis function and the batch gradient descent update rule remain unchanged. The code in the previous single variable part already supports multiple variables and can be used here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data2.columns)-1\n",
    "x, y, theta = prepareData(data2, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize learning rate α.\n",
    "alpha = 0.15\n",
    "\n",
    "# Check the dimensions of the matrices.\n",
    "x.shape, y.shape, theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Learning Rates\n",
    "\n",
    "A learning rate that converges quickly shall be found. Gradient descent will be run for 50 iterations at the chosen learning rate. The `gradientDescent` function also returns the history of $J(\\theta)$ values in a vector `J_vals`. Finally, the J values are plotted against the number of the iterations. A learning rate within a good range is depicted in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, J_vals = gradientDescent(x, theta, iterations=50)\n",
    "\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Convergence of gradient descent with an appropriate learning rate', y=1.08)\n",
    "plt.grid()\n",
    "plt.plot(range(50), J_vals, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction\n",
    "\n",
    "Make a price prediction for a 1650-square-foot house with 3 bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the price for a 1650-square-foot house with 3 bedrooms.\n",
    "# First normalize features using the std and mean values.\n",
    "size = (1650 - 2000.680851) / 794.702354\n",
    "bedrooms = (3 - 3.170213) / 0.760982\n",
    "theta_f = list(theta.flat)\n",
    "print('Price: $', (theta_f[0] + theta_f[1] * size + theta_f[2] * bedrooms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "The closed-form solution to linear regression is\n",
    "\n",
    "$\\theta=(X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "This formula does not require any feature scaling and gives an exact solution in one calculation: there is no “loop until convergence” like in gradient descent. It is still needed to add a column of 1’s to the $X$ matrix to have an intercept term $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "data2 = pd.read_csv('ex1data2.txt', names=['Size', 'Bedrooms', 'Price'])\n",
    "n = len(data2.columns)-1\n",
    "x, y, theta = prepareData(data2, n)\n",
    "\n",
    "# Create the normal equation.\n",
    "def normalEquation(x, y):\n",
    "    \"\"\"\n",
    "    Get the analytical solution to linear regression,\n",
    "    using the normal equation.\n",
    "    Args:\n",
    "        x: a m by n+1 matrix\n",
    "        y: a m by 1 vector\n",
    "    Returns:\n",
    "        theta: a n+1 by 1 vector\n",
    "    \"\"\"\n",
    "    theta = np.dot(np.dot(inv(np.dot(x.T, x)), x.T), y)\n",
    "    return theta\n",
    "\n",
    "theta = normalEquation(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Prediction\n",
    "\n",
    "Make the same prediction for a 1650-square-foot house with 3 bedrooms to check the analytical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to normalize the features now!\n",
    "theta_f = list(theta.flat)\n",
    "print('Price: $', (theta_f[0] + theta_f[1] * 1650 + theta_f[2] * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was expected, the predicted price with the normal equation is pretty similar to the one using the model fit with gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
